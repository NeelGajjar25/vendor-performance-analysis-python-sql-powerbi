{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "561c08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf48ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine= create_engine('sqlite:///inventory.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b06c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.13.8)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Neel/OneDrive/Desktop/Vendor Performance Data Analytics/.venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# 1. Define the Database Engine (assuming this is done earlier in your script)\n",
    "# Replace the URI with your actual database connection string\n",
    "engine = create_engine('sqlite:///inventory.db') \n",
    "\n",
    "# 2. Set the chunk size for both reading and writing\n",
    "# 50,000 rows is a good starting point for large files\n",
    "CHUNK_SIZE = 50000 \n",
    "\n",
    "for file in os.listdir('data'):\n",
    "    if '.csv' in file:\n",
    "        file_path = os.path.join('data', file)\n",
    "        table_name = file[:-4]  # Remove the last 4 characters (.csv)\n",
    "        \n",
    "        # This flag controls whether to replace the table or append to it\n",
    "        if_exists_mode = 'replace' \n",
    "        \n",
    "        print(f\"\\n--- Starting Ingestion: {file} into table '{table_name}' ---\")\n",
    "        \n",
    "        try:\n",
    "            # A. Read the CSV in chunks (pd.read_csv returns an iterator)\n",
    "            # This is the main change to fix the MemoryError during file reading.\n",
    "            chunk_iterator = pd.read_csv(file_path, chunksize=CHUNK_SIZE)\n",
    "            \n",
    "            for i, chunk_df in enumerate(chunk_iterator):\n",
    "                print(f\"  -> Processing chunk {i+1} with {len(chunk_df)} rows...\")\n",
    "                \n",
    "                # B. Ingest the current chunk into the database\n",
    "                chunk_df.to_sql(\n",
    "                    name=table_name,\n",
    "                    con=engine,\n",
    "                    if_exists=if_exists_mode, # Use 'replace' for the first chunk, then 'append'\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                # C. Switch the mode to 'append' for all subsequent chunks\n",
    "                if_exists_mode = 'append'\n",
    "                \n",
    "            print(f\"--- Successfully finished {file} ---\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catching errors related to the file or connection\n",
    "            print(f\"!!! Error processing file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05abb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
